# JobSearcherV2
Second version. Improvements with respect to V1:
- Uses selenium + scrapy for faster scraping.
- Sqlite3 for faster searches and DB management.
- More modular and self-explanatory scripts. The structure is clearer now.
- The only scraped page is Bumeran.com. I have others, but they aren't adapted to scrapy,
 so I am leaving them for future versions.
- All the parameters for the program are in project_constants.py. This way, main.py doesn't need to be modified.

All the project is in bumeran_scraper and it can be run by calling main.py.

# Requirements
I haven't listed them, maybe in a future version. Anyways, this could be helpful for someone stuck with some related project.
This project uses selenium, scrapy, pandas, chromedriver and sqlite3.

# Description
*****************************
About DB - It has two tables:
- Data (Columns: ['id', 'job', 'company', 'location', 'date', 'site', 'opened', 'link'])
- Search (Columns: ['id', 'keyword', 'site', 'update_date']
Id is the primary key, autogenerated. It's useful for erasing duplicates, so I can directly upload
data to the DB with pandas methods.
*****************************
About classes and their methods:
Since the code is self-explanatory, I will not go in depth. I will describe the scripts briefly,
and the ones I will not describe are self-generated by scrapy.
- bumeran_scraper (main folder):
    
    - bumeran_control.py: Has all the webpage control and data cleaning for dealing with Bumeran. 
     Since the site is dynamic, it needs Selenium to work. Also, the page blocks bots, so an 
     user_agent needs to be used. Another security measure it has is a periodic xPath change, so 
     it is updated automatically.
     
    - items.csv and tmp.csv: CSVs for developing, don't take them into account. Maybe you could
      see them to know how the output looks like.
      
    - main.py: The main program. From here you call the scraper, update the DB and perform some
     search with the option of opening all links in a chromium window. 
     
     - project_constants.py: The parameters for main.py and other scripts in this project.
     They must be modified here, so you don't need to modify main.py.
     
    - sql_control.py: Has all the control related to sqlite3, create and update databases or
    perform some searches on it. The search feature includes keyword groups of AND logic and 
    OR logic, and a MAX_DAYS to not get job postings from late dates.
    
    - system_tools.py: This has an ugly bug in which chromedriver keeps opened. I could close them
    in the main script but I don't want to because it would be harmful for future versions. When you
    finish using this, you should run this script to close all chromedrivers.
    
- bumeran_scraper/bumeran_scraper (one level down):
    
    - items.py: Autogenerated by scrapy, but modified to create the Item object it requires, with
    the necessary columns for scraping.
    
    - pipelines.py and settings.py: Pipelines.py is used to perform data cleaning when data is incoming
    from the crawler. It doesn't have too much code since all the cleaning is implemented in bumeran_control.py.
    Also, due to using pipelines.py, need to modify settings.py like it says in the description.

- bumeran_scraper/bumeran_scraper/spiders (one level down):
    
    - basic.py: Autogenerated by scrapy, but with some modifications. Performs a fake request to
    call the parse method which uses the bumeran_control classes to go through the webpage, and scrapy to
    process the data. It's much faster this way, I have a selenium only scraper and probably this is x5 faster.
    The thing that may need some refactoring is the bad part of using a lot of generators to yield the items.
    Anyways, the code is understandable.
    
